{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Zeros of Real Functions\n",
    "\n",
    "**by Roberto E. Navarro**\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This document discusses some methods to address the problem of finding zeros of a real function $f(x)$, or equivalently, locating the points $x = x^*$ where $f(x^*) = 0$. In many cases, $f(x)$ may be a non-algebraic or transcendental function, for which its zeros or roots are not analytical or do not have a closed form. Most methods for finding roots are iterative, that is, starting from an initial estimate $x = x_0$, we expect that an appropriate sequence $x_k$ will iteratively approach the root such that $x_k \\to x^*$ for some $k > K$. A large number of methods have been developed, mainly because none of them work in all cases, so the method chosen highly depends on the type of function $f(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Bisection Method\n",
    "\n",
    "An initial approach could be to try to bound the roots of $f(x)$. If $f(x)$ is continuous in a certain interval $a < x < b$ and if $f(a)f(b) < 0$, that is, $f(x)$ changes its sign, then we can ensure that there exists a certain value $a < x^* < b$ in the interval such that $f(x^*) = 0$.\n",
    "\n",
    "Thus, the following algorithm is proposed:\n",
    "\n",
    "1. If $f(a) = 0$, then $x = a$ is a root, and the algorithm terminates. The same applies to $f(b)$.\n",
    "2. Define $c = (a + b)/2$.\n",
    "3. If $f(a)f(c) < 0$, then $a < x^* < c$ is a new, narrower interval. Then, $b = c$.\n",
    "4. Otherwise, $c < x^* < b$ is the new interval. Then, $a = c$.\n",
    "5. Repeat the cycle from step (1) until a root is found or $|b - a| < \\epsilon$ with $\\epsilon$ being a certain tolerance.\n",
    "\n",
    "Note that, if $c_n = (a_n + b_n)/2$ in the $n$-th iteration, with $a_n = \\{a_{n-1}, c_{n-1}\\}$ and $b_n = \\{c_{n-1}, b_{n-1}\\}$, then:\n",
    "\n",
    "$$\n",
    "c_n - x^* \\approx \\frac{1}{2}(b_n - a_n) = \\frac{1}{2^{n+1}}(b_0 - a_0)\n",
    "$$\n",
    "\n",
    "Therefore, $c_n \\to x^*$ when $n \\to \\infty$. This also implies that, if $\\epsilon_n = c_n - x^*$, then:\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} \\approx \\frac{1}{2}\\epsilon_n\n",
    "$$\n",
    "\n",
    "When the error $\\epsilon_n$ of the method satisfies a rule of the form $\\epsilon_{n+1} \\propto \\epsilon_n^p$, the method is said to be of order $p$. In the case of the bisection method, its order is $p = 1$.\n",
    "\n",
    "# 2 Newton-Raphson Method\n",
    "\n",
    "Let us assume that $x = x_n$ is sufficiently close to the root $x = x^*$. Then, we expand around $x = x_n$ such that\n",
    "\n",
    "$$\n",
    "f(x^*) \\simeq f(x_n) + f'(x_n)(x^* - x_n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^* \\simeq x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "$$\n",
    "\n",
    "If $x^*$ is not exactly a root, then we define the Newton-Raphson method as:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "$$\n",
    "\n",
    "and we repeat the process until $x_n \\to x^*$ converges to the root.\n",
    "\n",
    "To estimate the order of convergence, we use a similar expansion but around the root:\n",
    "\n",
    "$$\n",
    "f(x_n) = f'(x^*)\\epsilon_n + \\frac{1}{2}f''(x^*)\\epsilon_n^2 + O(\\epsilon_n^3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(x_n) = f'(x^*) + f''(x^*)\\epsilon_n + O(\\epsilon_n^2)\n",
    "$$\n",
    "\n",
    "where $\\epsilon_n = x_n - x^*$. Replacing it in the Newton-Raphson method:\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} = \\epsilon_n - \\frac{f'(x^*)\\epsilon_n + \\frac{1}{2}f''(x^*)\\epsilon_n^2 + O(\\epsilon_n^3)}{f'(x^*) + f''(x^*)\\epsilon_n + \\frac{1}{2}f'''(x^*)\\epsilon_n^2 + O(\\epsilon_n^3)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} = \\epsilon_n - \\frac{f'(x^*)\\epsilon_n + \\frac{1}{2}f''(x^*)\\epsilon_n^2 + O(\\epsilon_n^3)}{f'(x^*)\\left[1 + \\frac{f''(x^*)}{f'(x^*)}\\epsilon_n + \\frac{1}{2}\\frac{f'''(x^*)}{f'(x^*)}\\epsilon_n^2 + O(\\epsilon_n^3)\\right]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} = \\epsilon_n - \\frac{\\frac{f'(x^*)\\epsilon_n + \\frac{1}{2}f''(x^*)\\epsilon_n^2 + O(\\epsilon_n^3)}{f'(x^*)}}{1 + \\frac{f''(x^*)}{f'(x^*)}\\epsilon_n + O(\\epsilon_n^2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} \\simeq \\frac{1}{2}\\frac{f''(x^*)}{f'(x^*)}\\epsilon_n^2 + O(\\epsilon_n^3)\n",
    "$$\n",
    "\n",
    "Thus, since $\\epsilon_{n+1} \\propto \\epsilon_n^2$, the order of the method is 2.\n",
    "\n",
    "In the second line of the previous procedure, we factorized the denominator by $f'(x^*)$. In the third line, we used the approximation of the geometric series $1/(1 + \\alpha\\epsilon) \\simeq 1 - \\alpha\\epsilon$.\n",
    "\n",
    "Note that terms of the form $O(\\epsilon^n)$ correspond to the big-O notation, which is simply an indication that there are terms of power $\\epsilon^n$ or higher, with $\\epsilon^n$ being the dominant term. Thus:\n",
    "\n",
    "1. If $O(\\epsilon^m)$ are terms of order $m$, then $\\beta\\epsilon^n O(\\epsilon^m) = O(\\epsilon^{m+n})$ is simply a term of order $m + n$, with $\\beta$ a constant or function independent of $\\epsilon$.\n",
    "   \n",
    "2. The remainder $O(\\epsilon^n) - O(\\epsilon^n)$ is not necessarily zero. Recall that $O(\\epsilon^n)$ only indicates the order of an expression, but it does not specify its exact form. Thus, $O(\\epsilon^n) - O(\\epsilon^n)$ could represent the remainder of polynomial series like $\\alpha\\epsilon^n - \\beta\\epsilon^n = (\\alpha - \\beta)\\epsilon^n$, which results in another polynomial of order $n$ and only cancels out when $\\alpha = \\beta$. As we generally have little information about the coefficients, $O(\\epsilon^n) - O(\\epsilon^n) = O(\\epsilon^n)$ is another term of order $n$.\n",
    "\n",
    "3. The sum $O(\\epsilon^m) + O(\\epsilon^n)$ must evaluate which term is dominant. If $n < m$, then $O(\\epsilon^n)$ (the smaller order) is dominant over $O(\\epsilon^m)$ (the larger order), so $O(\\epsilon^n) + O(\\epsilon^m) = O(\\epsilon^n)$. In general, $O(\\epsilon^m) + O(\\epsilon^n) = O(\\epsilon^{\\min(m,n)})$.\n",
    "\n",
    "**NOTE:** If $f(x^*) = 0$ and additionally $f'(x^*) = 0$, that is, $x = x^*$ is both a root and an extremum of $f$ (the root is said to have multiplicity 2), then:\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} = \\epsilon_n - \\frac{\\frac{1}{2}f''(x^*)\\epsilon_n^2 + \\frac{1}{6}f'''(x^*)\\epsilon_n^3 + O(\\epsilon_n^4)}{f''(x^*)\\epsilon_n + \\frac{1}{2}f'''(x^*)\\epsilon_n^2 + O(\\epsilon_n^3)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} \\simeq \\epsilon_n - \\frac{\\frac{1}{2}f''(x^*)\\epsilon_n^2 + \\frac{1}{6}f'''(x^*)\\epsilon_n^3 + O(\\epsilon_n^4)}{f''(x^*)\\left[1 + \\frac{1}{2}\\frac{f'''(x^*)}{f''(x^*)}\\epsilon_n + O(\\epsilon_n^2)\\right]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+1} \\simeq \\frac{1}{2}\\epsilon_n + \\frac{\\frac{1}{12}f'''(x^*)}{f''(x^*)}\\epsilon_n^2 + O(\\epsilon_n^3)\n",
    "$$\n",
    "\n",
    "Therefore, in this case, the convergence is at most linear, and it may be necessary to consider the second derivative for Newton's method.\n",
    "\n",
    "# 3 Secant Method\n",
    "\n",
    "$$\n",
    "x_{n+2} = x_n - \\frac{f(x_n)}{f(x_{n+1}) - f(x_n)}(x_{n+1} - x_n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_{n+1})}\n",
    "$$\n",
    "\n",
    "If $x_n$ is sufficiently close to the root $x = r$ such that $f(r) = 0$, then:\n",
    "\n",
    "$$\n",
    "f(x_n) = f(r) + f'(r)(x_n - r) + \\frac{1}{2}f''(r)(x_n - r)^2 + \\cdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(x_{n+1}) = f(r) + f'(r)(x_{n+1} - r) + \\frac{1}{2}f''(r)(x_{n+1} - r)^2 + \\cdots\n",
    "$$\n",
    "\n",
    "Substituting into the method:\n",
    "\n",
    "$$\n",
    "x_{n+2} = x_n - \\frac{f'(r)(x_n - r) + \\frac{1}{2}f''(r)(x_n - r)^2 + \\cdots}{f'(r)(x_{n+1} - r) + \\frac{1}{2}f''(r)(x_{n+1} - r)^2 + \\cdots} (x_{n+1} - x_n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= x_n - \\frac{f(r) + f'(r)(x_n - r) + \\frac{1}{2}f''(r)(x_n - r)^2 + \\cdots}{f'(r) + \\frac{1}{2}f''(r)[x_{n+1} + x_n - 2r] + \\cdots}\n",
    "$$\n",
    "\n",
    "Defining $\\epsilon_n = x_n - r$:\n",
    "\n",
    "$$\n",
    "\\epsilon_{n+2} \\approx \\epsilon_n - \\frac{f'(r) + \\frac{1}{2}f''(r)\\epsilon_n}{f'(r) + \\frac{1}{2}f''(r)(\\epsilon_{n+1} + \\epsilon_n)}\\epsilon_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \\epsilon_n - \\epsilon_n\\left[1 + \\frac{1}{2}\\frac{f''(r)}{f'(r)}\\epsilon_n\\right]^{-1}\\left[1 - \\frac{1}{2}\\frac{f''(r)}{f'(r)}(\\epsilon_{n+1} + \\epsilon_n)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \\epsilon_n - \\left[\\epsilon_n + \\frac{1}{2}\\frac{f''(r)}{f'(r)}\\epsilon_n^2\\right] + \\epsilon_n\\left[\\frac{1}{2}\\frac{f''(r)}{f'(r)}\\epsilon_n\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\frac{f''(r)}{f'(r)}\\epsilon_{n+1}\\epsilon_n\n",
    "$$\n",
    "\n",
    "Suppose that $\\epsilon_{n+1} = k\\epsilon_n^p$ and $\\epsilon_{n+2} = k\\epsilon_{n+1}^p = k^{p+1}\\epsilon_n^{p^2}$, then\n",
    "\n",
    "$$\n",
    "k^{p+1}\\epsilon_n^{p^2} \\approx \\frac{1}{2}\\frac{f''(r)}{f'(r)}k^{p+1}\\epsilon_n^{p+1}\n",
    "$$\n",
    "\n",
    "with solutions\n",
    "\n",
    "$$\n",
    "k^p = \\frac{1}{2}\\frac{f''(r)}{f'(r)}\n",
    "$$\n",
    "\n",
    "or $p^2 = p + 1$.\n",
    "\n",
    "Thus, $p = \\frac{(1 + \\sqrt{5})}{2} \\approx 1.618$, which is the golden ratio. Therefore, $\\epsilon_{n+1} = k\\epsilon_n^{1.618}$, and the method has an order of $1.618$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
